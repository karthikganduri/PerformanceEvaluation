# PerformanceEvaluation
OpenAI-compatible LLM performance benchmarking toolkit that runs 3 prompt types across 100/1k/10k-token inputs and reports TTFT, time-to-first-reasoning-token, OTPS, and total latency, with clear flags when a model does not support larger context sizes.
